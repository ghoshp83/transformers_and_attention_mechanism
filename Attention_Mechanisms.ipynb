{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms \n",
    "\n",
    "This notebook provides an overview of the attention mechanism in machine learning models, particularly for tasks like machine translation. It explains how attention allows the decoder to focus on the most relevant parts of the input sequence. Key concepts like queries, keys, and values are illustrated with clear examples and step-by-step implementation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries/packages used\n",
    "\n",
    "* [<code style=\"color:blue;\">numpy:</code>](https://numpy.org/doc/) A powerful library for numerical computing, providing support for multi-dimensional arrays, mathematical functions, and array operations\n",
    "\n",
    "* [<code style=\"color:blue;\">scipy:</code>](https://docs.scipy.org/doc/)  A library that provides additional scientific computing functionalities\n",
    "\n",
    "To run this code on your local machine, please run the following commands in your command line tool.\n",
    "* `pip install numpy`\n",
    "* `pip install scipy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>  \n",
    "## Table of contents  \n",
    "\n",
    "1. <a href=\"#attention_mechanism\">Attention mechanism</a><br>  \n",
    "2. <a href=\"#required_imports\">Required imports</a><br>  \n",
    "3. <a href=\"#word_embeddings\">Word embeddings</a><br>  \n",
    "4. <a href=\"#weight_matrices\">Creating weight matrices</a><br>  \n",
    "5. <a href=\"#queries_keys_values\">Queries, keys, and values</a><br>  \n",
    "6. <a href=\"#calculating_scores\">Calculating scores</a><br>  \n",
    "7. <a href=\"#softmax_weights\">Applying softmax weights</a><br>  \n",
    "8. <a href=\"#calculating_attention\">Calculating attention</a><br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attention_mechanism'></a>  \n",
    "## 1. Attention mechanism  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "The attention mechanism enhances the encoder-decoder model by allowing the decoder to focus on the most relevant parts of the input sequence. It computes weighted sums of the input vectors, giving higher weights to the most relevant components.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='required_imports'></a>  \n",
    "## 2. Required imports  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "Below are the functions needed for this implementation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.288282Z",
     "start_time": "2025-10-07T21:48:37.284831Z"
    }
   },
   "source": [
    "# importing required functions  \n",
    "from numpy import array  # for creating and handling arrays  \n",
    "from numpy import random  # for generating random numbers  \n",
    "from scipy.special import softmax  # for applying the softmax function"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word_embeddings'></a>  \n",
    "## 3. Word embeddings  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "Here we define the word embeddings for four words, which we will later use to compute attention. In real-world applications, these word embeddings would typically be generated by an encoder. However, for this specific example, we will manually define them.\n",
    "\n",
    "* [<code style=\"color:blue;\">array</code>](https://numpy.org/doc/stable/reference/generated/numpy.array.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.335983Z",
     "start_time": "2025-10-07T21:48:37.333427Z"
    }
   },
   "source": [
    "# manually defining word embeddings for four words  \n",
    "word_1 = array([1, 0, 0])  \n",
    "word_2 = array([0, 1, 0])  \n",
    "word_3 = array([1, 1, 0])  \n",
    "word_4 = array([0, 0, 1])  "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.391538Z",
     "start_time": "2025-10-07T21:48:37.386566Z"
    }
   },
   "source": [
    "# stacking all word embeddings into one array  \n",
    "words = array([word_1, word_2, word_3, word_4])  "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.450870Z",
     "start_time": "2025-10-07T21:48:37.445850Z"
    }
   },
   "source": [
    "# printing the word embeddings  \n",
    "print(\"Word embeddings:\\n\", words)  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings:\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='weight_matrices'></a>  \n",
    "## 4. Creating weight matrices  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "This step involves creating the weight matrices that will be used to produce the queries, keys, and values from the word embeddings. In this context, we are generating these weight matrices in a random manner. However, it's worth noting that in real-world applications, these matrices would typically be learned and fine-tuned through training processes.\n",
    "\n",
    "* [<code style=\"color:blue;\">random</code>](https://numpy.org/doc/stable/reference/random/index.html)  \n",
    "\n",
    "* [<code style=\"color:blue;\">randint</code>](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html)  \n",
    "* [<code style=\"color:blue;\">seed</code>](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html)  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.509238Z",
     "start_time": "2025-10-07T21:48:37.504854Z"
    }
   },
   "source": [
    "# setting a seed for reproducibility  \n",
    "random.seed(42)  \n",
    "\n",
    "# defining weight matrices for query, key, and value  \n",
    "W_Q = random.randint(3, size=(3, 3))  # query weights  \n",
    "W_K = random.randint(3, size=(3, 3))  # key weights  \n",
    "W_V = random.randint(3, size=(3, 3))  # value weights  "
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.560317Z",
     "start_time": "2025-10-07T21:48:37.557238Z"
    }
   },
   "source": [
    "# printing the weight matrices  \n",
    "print(\"Query weights (W_Q):\\n\", W_Q)  \n",
    "print(\"Key weights (W_K):\\n\", W_K)  \n",
    "print(\"Value weights (W_V):\\n\", W_V)  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weights (W_Q):\n",
      " [[2 0 2]\n",
      " [2 0 0]\n",
      " [2 1 2]]\n",
      "Key weights (W_K):\n",
      " [[2 2 2]\n",
      " [0 2 1]\n",
      " [0 1 1]]\n",
      "Value weights (W_V):\n",
      " [[1 1 0]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jU1h6c8EG7P5"
   },
   "source": [
    "<a id='queries_keys_values'></a>  \n",
    "## 5. Queries, keys, and values  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "The **query (Q)**, **key (K)**, and **value (V)** concept can be compared to how retrieval systems work. For instance, when you search for videos on YouTube, your **query (Q)** — the text entered in the search bar—is compared against a set of **keys (K)**, such as video titles, descriptions, or metadata, stored in the database. The system then retrieves and presents the most relevant results as **values (V)** — the videos themselves.\n",
    "\n",
    "* [<code style=\"color:blue;\">@ (Matrix Multiplication)</code>](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.610203Z",
     "start_time": "2025-10-07T21:48:37.607079Z"
    }
   },
   "source": [
    "# computing query, key, and value matrices  \n",
    "Q = words @ W_Q  # query  \n",
    "K = words @ W_K  # key  \n",
    "V = words @ W_V  # value  \n",
    "\n",
    "# printing the matrices  \n",
    "print(\"Query matrix (Q):\\n\", Q)  \n",
    "print(\"Key matrix (K):\\n\", K)  \n",
    "print(\"Value matrix (V):\\n\", V)  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query matrix (Q):\n",
      " [[2 0 2]\n",
      " [2 0 0]\n",
      " [4 0 2]\n",
      " [2 1 2]]\n",
      "Key matrix (K):\n",
      " [[2 2 2]\n",
      " [0 2 1]\n",
      " [2 4 3]\n",
      " [0 1 1]]\n",
      "Value matrix (V):\n",
      " [[1 1 0]\n",
      " [0 1 1]\n",
      " [1 2 1]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKzL1bxFLeyn"
   },
   "source": [
    "<a id='calculating_scores'></a>  \n",
    "## 6. Calculating scores  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "The score determines the level of focus placed on different parts of the input sentence when encoding a word at a specific position. It is calculated by taking the dot product of the query vector with the key vector corresponding to the word being scored.\n",
    "\n",
    "* [<code style=\"color:blue;\">transpose</code>](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.transpose.html)  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aGCeysWD7Dnw",
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.661028Z",
     "start_time": "2025-10-07T21:48:37.657581Z"
    }
   },
   "source": [
    "# calculating attention scores  \n",
    "scores = Q @ K.transpose()  \n",
    "\n",
    "# printing the attention scores  \n",
    "print(\"Attention scores:\\n\", scores) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores:\n",
      " [[ 8  2 10  2]\n",
      " [ 4  0  4  0]\n",
      " [12  2 14  2]\n",
      " [10  4 14  3]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoogwnIYO69s"
   },
   "source": [
    "<a id='softmax_weights'></a>  \n",
    "## 7. Applying softmax weights  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "Now, we'll adjust the scores by dividing them by the square root of the key matrix's dimensionality, which helps stabilize gradients. The `softmax` function then ensures that the scores become positive and collectively sum to 1. These softmax scores dictate the relative emphasis each word will have at this particular position.\n",
    "\n",
    "* [<code style=\"color:blue;\">softmax</code>](https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.special.softmax.html)  \n",
    "* [<code style=\"color:blue;\">shape</code>](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html)  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mPWdxtPl7IE8",
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.711581Z",
     "start_time": "2025-10-07T21:48:37.707399Z"
    }
   },
   "source": [
    "# scaling and applying softmax to the scores  \n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)  # axis=1 means row-wise  \n",
    "\n",
    "# printing the attention weights  \n",
    "print(\"Attention weights:\\n\", weights)  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " [[2.36089863e-01 7.38987555e-03 7.49130386e-01 7.38987555e-03]\n",
      " [4.54826323e-01 4.51736775e-02 4.54826323e-01 4.51736775e-02]\n",
      " [2.39275049e-01 7.43870015e-04 7.59237211e-01 7.43870015e-04]\n",
      " [8.99501754e-02 2.81554063e-03 9.05653685e-01 1.58059922e-03]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50iUcJtERYRV"
   },
   "source": [
    "<a id='calculating_attention'></a>  \n",
    "## 8. Calculating attention  \n",
    "[Back to table of contents](#TOC)  \n",
    "\n",
    "We'll perform a multiplication between the value matrix and the softmax scores. The idea behind this is to preserve the original values of the word(s) we intend to emphasize or pay attention to."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RfqVFlLj7RWX",
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.763653Z",
     "start_time": "2025-10-07T21:48:37.759314Z"
    }
   },
   "source": [
    "# calculating attention output  \n",
    "attention = weights @ V  \n",
    "\n",
    "# printing the attention output  \n",
    "print(\"Attention output:\\n\", attention) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:\n",
      " [[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UrN25Ew7cak",
    "outputId": "a27e9b4e-a2be-4742-f742-2d3718960dfb",
    "ExecuteTime": {
     "end_time": "2025-10-07T21:48:37.814591Z",
     "start_time": "2025-10-07T21:48:37.811138Z"
    }
   },
   "source": [
    "# printing the final attention output  \n",
    "print(\"Final attention output:\\n\", attention)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final attention output:\n",
      " [[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
