{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e659934d",
   "metadata": {},
   "source": [
    "# Attention Scoring Functions\n",
    "\n",
    "Attention mechanisms are vital in deep learning, especially in tasks like NLP and computer vision. They enable models to focus on relevant parts of input data.  \n",
    "This notebook explores key attention scoring functions:  \n",
    "- Dot product  \n",
    "- Scaled dot product  \n",
    "- Additive attention  \n",
    "\n",
    "This will help to gain insights into the foundation of models like transformers.[Reference](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712c04f",
   "metadata": {},
   "source": [
    "### Libraries/packages used  \n",
    "\n",
    "The following libraries are used in this notebook:  \n",
    "\n",
    "* [<code style=\"color:blue;\">numpy:</code>](https://numpy.org/doc/stable/) A library for numerical computations, enabling array manipulation and mathematical operations  \n",
    "* [<code style=\"color:blue;\">matplotlib:</code>](https://matplotlib.org/stable/) A plotting library used for creating visualizations of attention scores and patterns  \n",
    "* [<code style=\"color:blue;\">pytorch:</code>](https://pytorch.org/docs/stable/index.html) A popular deep learning framework for building neural networks\n",
    "* [<code style=\"color:blue;\">d2l (dive into deep learning):</code>](https://d2l.ai/)  A library that simplifies implementation of deep learning concepts\n",
    "\n",
    "These packages are essential for implementing and visualizing attention scoring functions. To run this code on your local machine, please install the necessary libraries using the following commands:\n",
    "\n",
    "* `pip install torch`\n",
    "* `pip install d2l==1.0.3`\n",
    "* `pip install numpy ` \n",
    "* `pip install matplotlib ` \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c36d01a",
   "metadata": {},
   "source": [
    "!pip install d2l==1.0.3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba1bf70a",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>\n",
    "## Table of contents\n",
    "\n",
    "1. <a href=\"#intro\">Introduction</a><br>  \n",
    "2. <a href=\"#dot-product\">Dot product attention</a>  \n",
    "3. <a href=\"#convenience\">Convenience functions</a>  \n",
    "   3.1. <a href=\"#masked-softmax\">Masked softmax operation</a>  \n",
    "   3.2. <a href=\"#batch-matrix\">Batch matrix multiplication</a><br>  \n",
    "4. <a href=\"#dotproductattention-class\">Scaled dot product attention</a><br>\n",
    "5. <a href=\"#Additive\">Additive attention</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ae0ff",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "In attention pooling, various distance-based kernels, including Gaussian kernels, model interactions between queries and keys. While effective, distance functions are slightly more expensive to compute compared to dot products. To simplify computation, *attention scoring functions* are widely used. These functions are central to determining attention weights using softmax operations.\n",
    "\n",
    "![Computing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function $\\mathit{a}$ and the softmax operation.](img/attention-output.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd58e1",
   "metadata": {},
   "source": [
    "<a id='dot-product'></a>\n",
    "## 2. Dot product attention  \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "The attention function (without exponentiation) based on the Gaussian kernel is given as:\n",
    "\n",
    "$$\n",
    "a(\\mathbf{q}, \\mathbf{k}_i) = -\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}_i\\|^2  = \\mathbf{q}^\\top \\mathbf{k}_i -\\frac{1}{2} \\|\\mathbf{k}_i\\|^2  -\\frac{1}{2} \\|\\mathbf{q}\\|^2.\n",
    "$$\n",
    "\n",
    "First, note that the final term depends on $\\mathbf{q}$ only. As such it is identical for all $(\\mathbf{q}, \\mathbf{k}_i)$ pairs. Normalizing the attention weights to $1$, ensures that this term disappears entirely. \n",
    "\n",
    "Second, note that both batch and layer normalization (to be discussed later) lead to activations that have well-bounded, and often constant, norms $\\|\\mathbf{k}_i\\|$. This is the case, for instance, whenever the keys $\\mathbf{k}_i$ were generated by a layer norm. As such, we can drop it from the definition of $a$ without any major change in the outcome. \n",
    "\n",
    "Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathbb{R}^d$ and the key $\\mathbf{k}_i \\in \\mathbb{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains $1$ regardless of vector length, we use the `scaled dot product attention` scoring function. That is, we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used, e.g., in Transformers.\n",
    "\n",
    "$$ a(\\mathbf{q}, \\mathbf{k}_i) = \\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d}.$$\n",
    "\n",
    "\n",
    "Note that attention weights ùõºŒ± still need normalizing. We can simplify this further by using the softmax operation: \n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d})}{\\sum_{j=1} \\exp(\\mathbf{q}^\\top \\mathbf{k}_j / \\sqrt{d})}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534cd9bf",
   "metadata": {},
   "source": [
    "<a id='convenience'></a>\n",
    "## 3. Convenience functions  \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "There are a few utility functions needed to make the attention mechanism efficient to deploy. This includes tools for dealing with strings of variable lengths (common for natural language processing) and tools for efficient evaluation on minibatches (batch matrix multiplication). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58ca86",
   "metadata": {},
   "source": [
    "<a id='masked-softmax'></a>\n",
    "### 3.1 Masked softmax operation  \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "One of the most popular applications of the attention mechanism is to sequence models. Hence we need to be able to deal with sequences of different lengths. In some cases, such sequences may end up in the same minibatch, necessitating padding with dummy tokens for shorter sequences. These special tokens do not carry meaning. For instance, assume that we have the following three sentences:\n",
    "\n",
    "```\n",
    "Dive  into  Deep    Learning \n",
    "Learn to    code    <blank>\n",
    "Hello world <blank> <blank>\n",
    "```\n",
    "\n",
    "Since we do not want blanks in our attention model we simply need to limit $\\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ to $\\sum_{i=1}^l \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ for however long, $l \\leq n$, the actual sentence is. Since it is such a common problem, it has a name: the `masked softmax operation`. \n",
    "\n",
    "The implementation cheats ever so slightly by setting the values of $\\mathbf{v}_i$, for $i > l$, to zero. Moreover, it sets the attention weights to a large negative number, such as $-10^{6}$, in order to make their contribution to gradients and values vanish in practice. This is done since linear algebra kernels and operators are heavily optimized for GPUs and it is faster to be slightly wasteful in computation rather than to have code with conditional (if then else) statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8072f26",
   "metadata": {},
   "source": [
    "#### Example usage:\n",
    "To demonstrate how this function operates, imagine a minibatch with two examples of size $2 \\times 4$, where their valid lengths are $2$ and $3$, respectively. Due to the masked softmax operation, any values beyond the valid lengths for each vector pair are masked to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff4e21",
   "metadata": {},
   "source": [
    "If we need more fine-grained control to specify the valid length for each of the two vectors of every example, we simply use a two-dimensional tensor of valid lengths. This yields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed74ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79de6fd",
   "metadata": {},
   "source": [
    "<a id='batch-matrix'></a>\n",
    "### 3.2 Batch matrix multiplication  \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "Efficient computation of batch matrix multiplication (BMM) is critical for minibatches of queries, keys, and values.\n",
    "\n",
    "Another commonly used operation is to multiply batches of matrices by one another. This comes in handy when we have minibatches of queries, keys, and values. More specifically, assume that \n",
    "\n",
    "$$\\mathbf{Q} = [\\mathbf{Q}_1, \\mathbf{Q}_2, \\ldots, \\mathbf{Q}_n]  \\in \\mathbb{R}^{n \\times a \\times b}, \\\\\n",
    "    \\mathbf{K} = [\\mathbf{K}_1, \\mathbf{K}_2, \\ldots, \\mathbf{K}_n]  \\in \\mathbb{R}^{n \\times b \\times c}.\n",
    "$$\n",
    "\n",
    "Then the batch matrix multiplication (BMM) computes the elementwise product\n",
    "\n",
    "$$\\textrm{BMM}(\\mathbf{Q}, \\mathbf{K}) = [\\mathbf{Q}_1 \\mathbf{K}_1, \\mathbf{Q}_2 \\mathbf{K}_2, \\ldots, \\mathbf{Q}_n \\mathbf{K}_n] \\in \\mathbb{R}^{n \\times a \\times c}.$$\n",
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b76749",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.ones((2, 3, 4))\n",
    "K = torch.ones((2, 4, 6))\n",
    "d2l.check_shape(torch.bmm(Q, K), (2, 3, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f9e3d",
   "metadata": {},
   "source": [
    "<a id='dotproductattention-class'></a>\n",
    "## 4. Scaled dot product attention \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "In general, it requires that both the query and the key\n",
    "have the same vector length, say $d$, even though this can be addressed easily by replacing \n",
    "$\\mathbf{q}^\\top \\mathbf{k}$ with $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$ where $\\mathbf{M}$ is a matrix suitably chosen for translating between both spaces. For now assume that the dimensions match. \n",
    "\n",
    "In practice, we often think of minibatches for efficiency,\n",
    "such as computing attention for $n$ queries and $m$ key-value pairs,\n",
    "where queries and keys are of length $d$\n",
    "and values are of length $v$. The scaled dot product attention \n",
    "of queries $\\mathbf Q\\in\\mathbb R^{n\\times d}$,\n",
    "keys $\\mathbf K\\in\\mathbb R^{m\\times d}$,\n",
    "and values $\\mathbf V\\in\\mathbb R^{m\\times v}$\n",
    "thus can be written as \n",
    "$$\n",
    "\\mathrm{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}}\\right) \\mathbf{V} \\in \\mathbb{R}^{n \\times v}.\n",
    "$$\n",
    "\n",
    "Batch matrix multiplication ensures efficiency in this process. Additionally, dropout is used for regularization, helping to reduce overfitting. This approach enables scalable and stable attention computation for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):  #@save\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51786e",
   "metadata": {},
   "source": [
    "To demonstrate how the `DotProductAttention` class functions, we will use the same keys, values, and valid lengths from the previous toy example for additive attention. In this case, we assume a minibatch size of $2$, with a total of $10$ keys and values, each having a dimensionality of $4$. Additionally, the valid lengths for each observation are $2$ and $6$, respectively. Based on this setup, we expect the output to be a $2 \\times 1 \\times 4$ tensor, meaning one row per example in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbd97a",
   "metadata": {},
   "source": [
    "Let's check whether the attention weights actually vanish for anything beyond the second and sixth column respectively (because of setting the valid length to $2$ and $6$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493c178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d256b9",
   "metadata": {},
   "source": [
    "<a id='Additive'></a>\n",
    "## 5. Additive attention  \n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "When the queries $\\mathbf{q}$ and keys $\\mathbf{k}$ have different dimensions, we can either use a matrix $\\mathbf{M}$ to match their dimensions via $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$ or use additive attention as the scoring function. Additive attention is computationally more efficient due to its additive nature. Given a query $\\mathbf{q} \\in \\mathbb{R}^q$ and a key $\\mathbf{k} \\in \\mathbb{R}^k$, the additive attention scoring function is:\n",
    "\n",
    "$$\n",
    "a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}_v^\\top \\textrm{tanh}(\\mathbf{W}_q \\mathbf{q} + \\mathbf{W}_k \\mathbf{k}) \\in \\mathbb{R},\n",
    "$$\n",
    "where $\\mathbf{W}_q \\in \\mathbb{R}^{h \\times q}$, $\\mathbf{W}_k \\in \\mathbb{R}^{h \\times k}$, and $\\mathbf{w}_v \\in \\mathbb{R}^h$ are learnable parameters. This result is passed through a softmax for normalization and nonnegativity. An alternative interpretation is that the query and key are concatenated and passed through a multi-layer perceptron (MLP) with a hidden layer, using $\\tanh$ as the activation function and no bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):  #@save\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.w_v = nn.LazyLinear(1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
    "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
    "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of self.w_v, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
    "        # no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13bede",
   "metadata": {},
   "source": [
    "Let's see how `AdditiveAttention` works, in our toy example we pick queries, keys and values of size \n",
    "$(2, 1, 20)$, $(2, 10, 2)$ and $(2, 10, 4)$, respectively. This is identical to our choice for `DotProductAttention`, except that now the queries are $20$-dimensional. Likewise, we pick $(2, 6)$ as the valid lengths for the sequences in the minibatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04411161",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 20))\n",
    "\n",
    "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
    "attention.eval()\n",
    "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569185d",
   "metadata": {},
   "source": [
    "When reviewing the attention function we see a behavior that is qualitatively quite similar to that of `DotProductAttention`. That is, only terms within the chosen valid length $(2, 6)$ are nonzero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d32c83",
   "metadata": {},
   "source": [
    "**Summary:** In this section, we introduced the two primary attention scoring functions: dot product and additive attention, both of which are effective for aggregating sequences of varying lengths. Specifically, dot product attention is a core component of modern Transformer architectures. When queries and keys have different lengths, we can use the additive attention scoring function instead. Optimizing these layers has been a significant area of progress in recent years. For example, [NVIDIA's Transformer Library](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html) and [Megatron](https://arxiv.org/abs/1909.08053) rely on efficient variants of the attention mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
